"""
    File: openai_transcribe_wlv2.py

    Description: Given an input dataset and a pretrained model from hugging face (openai whisper-large-v2), it will transcribe audio clips in english

    Assumption: 
      - Available GPU memory of atleast 17GiB
      - Audio clips downsampled to 16kHz and converted to mono audio (Run modify_audio_clips.py first)

    Input: Directory path where input dataset is located, directory path where all audio clips are stored, modified audio clips directory path where audio clips are updated based on # of channels and sampling rate
           
    Output: Transcribed audio clips with other audio clip info

    Run: 
      python3 openai_transcribe_wlv2.py --input_dataset=/path/to/input/dataset --audio_clip_dir=/path/to/audio_clip_directory --modified_audio_clip_dir=/path/to/modified_audio_clip_directory --output_dataset=/path/to/store/dataset

    examples: 
      python3 openai_transcribe_wlv2.py --input_dataset=./dataset/dataset1.csv --audio_clip_dir=../../AudioClips_DataRepoService/ --modified_audio_clip_dir=./modified_audio_clips/ --output_dataset=./results/dataset1_whisper_large_v2.csv 
      python3 openai_transcribe_wlv2.py --input_dataset=./dataset/dataset2.csv --audio_clip_dir=../../AudioClips_DataRepoService/ --modified_audio_clip_dir=./modified_audio_clips/ --output_dataset=./results/dataset2_whisper_large_v2.csv 
      python3 openai_transcribe_wlv2.py --input_dataset=./dataset/dataset3.csv --audio_clip_dir=../../AudioClips_DataRepoService/ --modified_audio_clip_dir=./modified_audio_clips/ --output_dataset=./results/dataset3_whisper_large_v2.csv 
      python3 openai_transcribe_wlv2.py --input_dataset=../../YD_2.0_transcribed_audio_clips_english_v5.csv --audio_clip_dir=../../AudioClips_DataRepoService/ --modified_audio_clip_dir=./modified_audio_clips/ --output_dataset=./results/dataset_whisper_large_v2_exception.csv 
"""

from transformers import WhisperProcessor, WhisperForConditionalGeneration, pipeline
import soundfile as sf
import os
import pandas as pd
import argparse

HAVE_GPU = 1
if not HAVE_GPU: 
  device = "cpu"
else:
  device = "cuda"
print(device)

parser = argparse.ArgumentParser()

parser.add_argument('-i', '--input_dataset', type=str, help="Input dataset file path")
parser.add_argument('-a', '--audio_clip_dir', type=str, help="Path where all audio clips are stored")
parser.add_argument('-m', '--modified_audio_clip_dir', type=str, help="Path where all updated audio clips will be stored")
parser.add_argument('-o', '--output_dataset', type=str, help="Output dataset file path")

args = parser.parse_args()

input_file = args.input_dataset
audio_clip_dir = args.audio_clip_dir
modified_audio_clip_dir = args.modified_audio_clip_dir
output_file = args.output_dataset

count=0
save_interval = 5     # save data to output file after every 5 audio clips are transcribed

# read input dataset to fetch audio clips to be transcribed
input_df = pd.read_csv(input_file)
file_list = list(input_df['audio_clip_file_name'])
# while executing code for exception files 
# with open("exceptions.txt", "r") as f:
#   file_list = [line.strip() for line in f.readlines()]

# list path where all audio clips are stored
audio_files = [filename for filename in os.listdir(audio_clip_dir) if filename.endswith(".wav")]

# list path where all modified audio clips are stored
modified_audio_clip_dir = modified_audio_clip_dir
if not os.path.exists(modified_audio_clip_dir):
    os.makedirs(modified_audio_clip_dir)
modified_audio_clips = os.listdir(modified_audio_clip_dir)

# output file path to store audio clip transcript generated by the openai model
output_file_dir = './results'
if not os.path.exists(output_file_dir):
    os.makedirs(output_file_dir)
output_file_path = output_file
if not os.path.isfile(output_file_path):
  data = {'youdescribe_link': [], 'audio_clip_file_path':[], 'audio_clip_file_name':[], 'audio_clip_duration':[], 'audio_clip_start_time':[], 'audio_clip_end_time':[], 'audio_clip_playback_type':[], 'db_transcript':[], 'oa_transcript':[]}
  output_df = pd.DataFrame(data)
  output_df.to_csv(output_file_path, index=False, header=True)
else:
  output_df = pd.read_csv(output_file_path)

# load model and processor
model_name = "openai/whisper-large-v2"
processor = WhisperProcessor.from_pretrained(model_name)
if HAVE_GPU:
  model = WhisperForConditionalGeneration.from_pretrained(model_name).to(device)
else:
  model = WhisperForConditionalGeneration.from_pretrained(model_name)
model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language="english", task="transcribe")

# pipeline configuration
pipe = pipeline(
  "automatic-speech-recognition",
  model=model_name,
  chunk_length_s=30,
  device=device
)
file_list = ['14875064946041.wav', 'WUG-x1TFewA_5d924503d5be181566c3fde5.wav']
for audio_file in file_list:

  count+=1
  print(f"File {count}: {audio_file}")

  if audio_file in modified_audio_clips:

    audio_path = f"{modified_audio_clip_dir}{audio_file}"
    audio_input, _ = sf.read(audio_path)

  else:
    audio_path = f"{audio_clip_dir}{audio_file}"
    audio_input, _ = sf.read(audio_path)
  
  try:

    transcription = pipe(audio_input.copy(), batch_size = 8, return_timestamps=True)["chunks"]
    timestamps = [data_dict['timestamp'] for data_dict in transcription if 'timestamp' in data_dict]
    transcription_sentences = [data_dict['text'] for data_dict in transcription if 'text' in data_dict]
    transcription_sentence = "".join(transcription_sentences)

    transcript_list = []
    transcript_dict = {}
    for time, sentence in zip(timestamps, transcription_sentences):
      transcript_start_time = time[0]
      transcript_end_time = time[1]
      data = {
        "start_time": transcript_start_time,
        "end_time": transcript_end_time,
        "sentence": sentence
      }
      transcript_dict.update(data)
      transcript_list.append(transcript_dict.copy())
      
    # Fetching info about the audio clip
    youdescribe_link = input_df.loc[input_df['audio_clip_file_name']==audio_file, 'youdescribe_link'].tolist()[0]
    file_path = input_df.loc[input_df['audio_clip_file_name']==audio_file, 'audio_clip_file_path'].tolist()[0]
    start_time = input_df.loc[input_df['audio_clip_file_name']==audio_file, 'audio_clip_start_time'].tolist()[0]
    end_time = input_df.loc[input_df['audio_clip_file_name']==audio_file, 'audio_clip_end_time'].tolist()[0]
    duration = input_df.loc[input_df['audio_clip_file_name']==audio_file, 'audio_clip_duration'].tolist()[0]
    playback_type = input_df.loc[input_df['audio_clip_file_name']==audio_file, 'audio_clip_playback_type'].tolist()[0]
    db_transcript = input_df.loc[input_df['audio_clip_file_name']==audio_file, 'audio_clip_transcript'].tolist()[0]

    new_data = [{
      'youdescribe_link': youdescribe_link,
      'audio_clip_file_path': file_path,
      'audio_clip_file_name': audio_file,
      'audio_clip_duration': duration,
      'audio_clip_start_time': start_time,
      'audio_clip_end_time': end_time,
      'audio_clip_playback_type': playback_type,
      'db_transcript': db_transcript,
      'oa_transcript': transcript_list
    }]

    new_row_df = pd.DataFrame(new_data)
    # Append the new row
    output_df = pd.concat([output_df, new_row_df], ignore_index=True)

    if count % save_interval == 0:
        output_df.to_csv(output_file_path, index=False)

  except Exception as e:
    with open('./exceptions.txt', 'a') as exp:
      exp.write(audio_file+'\n')
      exp.close()
    print(f"{e}, {audio_file}")

output_df.to_csv(output_file_path, index=False)