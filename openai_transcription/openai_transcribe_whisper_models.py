"""
    File: openai_transcribe_whisper_models.py

    Description: Given an input dataset and a pretrained model from hugging face, it will transcribe audio clips in english

    Assumption: 
      - Available GPU memory of atleast 21GiB
      - Audio clips downsampled to 16kHz and converted to mono audio (Run modify_audio_clips.py first)

    Input: Directory path where input dataset is located, directory path where all audio clips are stored, modified audio clips directory path where audio clips are updated based on # of channels and sampling rate
           
    Output: Transcribed audio clips with other audio clip info

    Run: 
      python3 openai_transcribe_whisper_models.py --input_dataset=/path/to/input/dataset --audio_clip_dir=/path/to/audio_clip_directory --modified_audio_clip_dir=/path/to/modified_audio_clip_directory --output_dataset=/path/to/store/dataset

    examples: 
      python3 openai_transcribe_whisper_models.py --input_dataset=./dataset/dataset1.csv --audio_clip_dir=../../AudioClips_DataRepoService/ --modified_audio_clip_dir=./modified_audio_clips/ --output_dataset=./results/dataset1_whisper_models_comparison.csv 
      python3 openai_transcribe_whisper_models.py --input_dataset=./dataset/dataset2.csv --audio_clip_dir=../../AudioClips_DataRepoService/ --modified_audio_clip_dir=./modified_audio_clips/ --output_dataset=./results/dataset2_whisper_models_comparison.csv 
      python3 openai_transcribe_whisper_models.py --input_dataset=./dataset/dataset3.csv --audio_clip_dir=../../AudioClips_DataRepoService/ --modified_audio_clip_dir=./modified_audio_clips/ --output_dataset=./results/dataset3_whisper_models_comparison.csv 
"""

from transformers import WhisperProcessor, WhisperForConditionalGeneration, AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
import torch
import soundfile as sf
import os
import pandas as pd
import argparse
import pickle

HAVE_GPU = 1
if not HAVE_GPU: 
  device = "cpu"
else:
  device = "cuda"
print(device)

torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

parser = argparse.ArgumentParser()

parser.add_argument('-i', '--input_dataset', type=str, help="Input dataset file path")
parser.add_argument('-a', '--audio_clip_dir', type=str, help="Path where all audio clips are stored")
parser.add_argument('-m', '--modified_audio_clip_dir', type=str, help="Path where all updated audio clips will be stored")
parser.add_argument('-o', '--output_dataset', type=str, help="Output dataset file path")

args = parser.parse_args()

input_file = args.input_dataset
audio_clip_dir = args.audio_clip_dir
modified_audio_clip_dir = args.modified_audio_clip_dir
output_file = args.output_dataset

count=0
save_interval = 1     # save data to output file after every 5 audio clips are transcribed

# read input dataset to fetch audio clips to be transcribed
input_df = pd.read_csv(input_file)
file_list = list(input_df['audio_clip_file_name'])
# while executing code for exception files 
# with open("exceptions.txt", "r") as f:
#   file_list = [line.strip() for line in f.readlines()]

# list path where all audio clips are stored
audio_files = [filename for filename in os.listdir(audio_clip_dir) if filename.endswith(".wav")]

# list path where all modified audio clips are stored
modified_audio_clip_dir = modified_audio_clip_dir
if not os.path.exists(modified_audio_clip_dir):
    os.makedirs(modified_audio_clip_dir)
modified_audio_clips = os.listdir(modified_audio_clip_dir)

# output file path to store audio clip transcript generated by the openai model
output_file_dir = './results'
if not os.path.exists(output_file_dir):
    os.makedirs(output_file_dir)
output_file_path = output_file

# load model and processor
def load_model_processor(model_name):

  if model_name == "openai/whisper-large-v2" or model_name == "openai/whisper-large":

    processor = WhisperProcessor.from_pretrained(model_name)
    if HAVE_GPU:
      model = WhisperForConditionalGeneration.from_pretrained(model_name).to(device)
    else:
      model = WhisperForConditionalGeneration.from_pretrained(model_name)
    model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language="english", task="transcribe")

    # pipeline configuration
    pipe = pipeline(
      "automatic-speech-recognition",
      model=model_name,
      chunk_length_s=30,
      batch_size=8,
      return_timestamps=True,
      device=device
    )
    return model_name, pipe

  elif model_name == "distil-whisper/distil-large-v2" or model_name == "distil-whisper/distil-medium.en" or model_name == "openai/whisper-large-v3":

    processor = AutoProcessor.from_pretrained(model_name)
    if HAVE_GPU:
      model = AutoModelForSpeechSeq2Seq.from_pretrained(
        model_name, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
    ).to(device)
    else:
      model = AutoModelForSpeechSeq2Seq.from_pretrained(
        model_name, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
    )

    # pipeline configuration
    pipe = pipeline(
        "automatic-speech-recognition",
        model=model,
        tokenizer=processor.tokenizer,
        feature_extractor=processor.feature_extractor,
        max_new_tokens=128,
        chunk_length_s=15,
        batch_size=16,
        return_timestamps=True,
        torch_dtype=torch_dtype,
        device=device,
    )
    return model_name, pipe
  
def transcribe_audio(audio_input, pipe):

  transcription = pipe(audio_input.copy())["chunks"]
  timestamps = [data_dict['timestamp'] for data_dict in transcription if 'timestamp' in data_dict]
  transcription_sentences = [data_dict['text'] for data_dict in transcription if 'text' in data_dict]
  transcription_sentence = "".join(transcription_sentences)

  transcript_list = []
  transcript_dict = {}
  for time, sentence in zip(timestamps, transcription_sentences):
    transcript_start_time = time[0]
    transcript_end_time = time[1]
    data = {
      "start_time": transcript_start_time,
      "end_time": transcript_end_time,
      "sentence": sentence
    }
    transcript_dict.update(data)
    transcript_list.append(transcript_dict.copy())
  
  return transcription_sentence

model_names = ["openai/whisper-large-v2", "openai/whisper-large", "openai/whisper-large-v3", "distil-whisper/distil-large-v2", "distil-whisper/distil-medium.en"]

# Create an empty DataFrame to store the results
columns = ['youdescribe_link', 'audio_clip_file_path', 'audio_clip_file_name', 'audio_clip_start_time', 'audio_clip_end_time', 'audio_clip_duration','audio_clip_playback_type', 'db_transcript'] + model_names
results_df = pd.DataFrame(columns=columns)

# Try to load models from cache, otherwise load and cache them
try:
    with open("./model/models_cache.pkl", "rb") as cache_file:
        models = pickle.load(cache_file)
except FileNotFoundError:
    models = {model_name: load_model_processor(model_name) for model_name in model_names}
    with open("./model/models_cache.pkl", "wb") as cache_file:
        pickle.dump(models, cache_file)

for audio_file in file_list[:100]: # transcribing 100 audio clips

  count+=1
  print(f"File {count}: {audio_file}")

  # Fetching info about the audio clip
  audio_info = {
    "youdescribe_link": input_df.loc[input_df['audio_clip_file_name']==audio_file, 'youdescribe_link'].tolist()[0],
    "audio_clip_file_path": input_df.loc[input_df['audio_clip_file_name']==audio_file, 'audio_clip_file_path'].tolist()[0],
    "audio_clip_file_name": audio_file,
    "audio_clip_start_time": input_df.loc[input_df['audio_clip_file_name']==audio_file, 'audio_clip_start_time'].tolist()[0],
    "audio_clip_end_time": input_df.loc[input_df['audio_clip_file_name']==audio_file, 'audio_clip_end_time'].tolist()[0],
    "audio_clip_duration": input_df.loc[input_df['audio_clip_file_name']==audio_file, 'audio_clip_duration'].tolist()[0],
    "audio_clip_playback_type": input_df.loc[input_df['audio_clip_file_name']==audio_file, 'audio_clip_playback_type'].tolist()[0],
    "db_transcript": input_df.loc[input_df['audio_clip_file_name']==audio_file, 'audio_clip_transcript'].tolist()[0]
  }

  if audio_file in modified_audio_clips:

    audio_path = f"{modified_audio_clip_dir}{audio_file}"
    audio_input, _ = sf.read(audio_path)

  else:
    audio_path = f"{audio_clip_dir}{audio_file}"
    audio_input, _ = sf.read(audio_path)

  try:

    transcriptions = {model_name: transcribe_audio(audio_input, model[1]) for model_name, model in models.items()}

    # Combine audio info and transcriptions into a single dictionary
    result_row = {**audio_info, **transcriptions}
    new_row_df = pd.DataFrame([result_row])

    results_df = pd.concat([results_df, new_row_df], ignore_index=True)

    if count % save_interval == 0:
      results_df.to_csv(output_file_path, index=False)

  except Exception as e:
    with open('./exceptions_whisper_models_comparison.txt', 'a') as exp:
      exp.write(audio_file + '\n')
      exp.close()
    print(f"{e}, {audio_file}")

results_df.to_csv(output_file_path, index=False, header=True)